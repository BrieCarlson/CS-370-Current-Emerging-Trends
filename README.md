	If a human were to try to solve this maze, they would do so by trial and error. As humans attempt to complete the maze, they would analyze it and consider possible outcomes before executing any actions. This gives the human player an advantage in finding the right path and solving the maze without making many mistakes throughout the trial. The intelligent agent’s steps to solve this pathfinding problem would begin more randomly. The agent would not know the correct path to take and is likely not to find the correct path within the first trial, so we would need to allow multiple attempts to find an appropriate route. The agent’s steps include getting data for the start point, target point, and other possible points the agent will need to make. Then they would need to run through the course multiple times until finding the optimal path to solve the maze. Then once they have the shortest path, they can run through the course most optimally. 
	Both approaches are looking for the shortest, most optimal path for completing the maze, but each does so differently. The human can see with their eyes and possibly use past maze-solving experiences to analyze the best route before making a move. The agent would have to rely on the input data and learn through trial and error with a system of rewards and penalties to tell whether that choice was good or bad. Another key difference between the two would be the time it takes for each one to find the path to solve the maze. 
	The difference between exploitation and exploration would be the amount of searching that is done. With Exploitation, they would search every possible way and try every combination of paths for the ideal solution. Exploitation takes advantage of the agent's existing estimated value and opts for the selfish strategy to reap the largest rewards. “Exploitation is defined as a greedy approach in which agents try to get more rewards by using estimated value but not the actual value. So, in this technique, agents make the best decision based on current information.” (Yang, 2022). The agent may receive a lower reward since it is greedy with the estimated value rather than the actual value. Exploration is a long-term benefit idea, allowing the agent to increase its understanding of any activity that might produce a long-term advantage. Although the agent can choose to explore and identify new avenues of exploration, the ideal ratio of exploitation and exploration for this pathfinding problem is that the agent typically learns through various paths of exploitation. We must find a balance between conducting enough initial research to identify the best options, utilizing the best option to maximize total reward, and continuing to set aside a small probability to experiment with less-than-ideal and unexplored options in case they offer higher returns in the future.
	Reinforcement learning helps to determine the path to the goal by the agent through trial and error. “Reinforcement Learning is a family of algorithms and techniques used for Control (e.g. Robotics, Autonomous driving, etc..) and Decision making. These approaches solve problems that need to be expressed as a Markov Decision Process (MDP)” (Comi, 2022). Only when utilizing a testing approach to resolve this pathfinding challenge, can the agent determine the most effective route. By giving rewards for correct moves done by the agent, the agent will favor moves that get them closer to the end goal or treasure. 
	Several steps were involved in implementing deep Q learning using neural networks for this game, using advanced learning algorithms, creating learning agents, training environments, incentive systems, importing libraries, and environment-based testing of agents. When employing a neural network, going through the deep Q-learning implementation phases can assist in determining the best feasible sequence that navigates and has impressive results in getting to the treasure cell while increasing the reward.
 
Gulli, A., & Pal, S. (2017). Deep learning with Keras: Get to grips with the basics of Keras to implement fast and efficient deep-learning models. Packt Publishing, Limited.
Beysolow, Taweh II. (2019). Applied Reinforcement Learning with Python: With OpenAI Gym,
Tensorflow, and Keras. Apress L. P
Reinforcement learning – exploration vs exploitation tradeoff. AI ML Analytics. (2022, January 2). https://ai-ml-analytics.com/reinforcement-learning-exploration-vs-exploitation-tradeoff/
Deep reinforcement learning for maze solving¶. qmaze. (n.d.). Retrieved December 7, 2022, from https://samyzaf.com/ML/rl/qmaze.html
Yang, A. (2022, July 25). What is exploration vs. exploitation in reinforcement learning? Medium. https://medium.com/@angelina.yang/what-is-exploration-vs-exploitation-in-reinforcement-learning-a3b96dcc9503#:~:text=The%20exploration%2Dexploitation%20trade%2Doff,more%20('exploration')
Comi, M. (2022, June 8). How to teach an AI to play games: Deep Reinforcement Learning. Medium. https://towardsdatascience.com/how-to-teach-an-ai-to-play-games-deep-reinforcement-learning-28f9b920440a
